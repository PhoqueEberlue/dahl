```md
## Forward convolution

### Single sample

Input sample:

        RED                BLUE              GREEN
 ┌───┬───┬───┬───┐  ┌───┬───┬───┬───┐  ┌───┬───┬───┬───┐
 │ 2 │ 2 │ 2 │ 2 │  │ 3 │ 0 │ 0 │ 3 │  │ 4 │ 0 │ 4 │ 4 │
 ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  ├───┼───┼───┼───┤
 │ 2 │ 0 │ 0 │ 2 │  │ 0 │ 3 │ 3 │ 0 │  │ 0 │ 4 │ 0 │ 4 │
 ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  ├───┼───┼───┼───┤
 │ 2 │ 0 │ 0 │ 2 │  │ 0 │ 3 │ 3 │ 0 │  │ 4 │ 0 │ 4 │ 0 │
 ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  ├───┼───┼───┼───┤
 │ 2 │ 2 │ 2 │ 2 │  │ 3 │ 0 │ 0 │ 3 │  │ 4 │ 4 │ 0 │ 4 │
 └───┴───┴───┴───┘  └───┴───┴───┴───┘  └───┴───┴───┴───┘

Filters:

                                x: Filter size
                               ◄─────────►
 ┌ ─ ┬ ─ ┬ ─ ┐ ┌ ─ ┬ ─ ┬ ─ ┐  ┌ ─ ┬ ─ ┬ ─ ┐ ▲
   1   0   1     0   1   0      1   1   1   │
 ├ ─ ┼ ─ ┼ ─ ┤ ├ ─ ┼ ─ ┼ ─ ┤  ├ ─ ┼ ─ ┼ ─ ┤ │
   0   1   0     1   0   1      1   0   0   │ y: Filter size
 ├ ─ ┼ ─ ┼ ─ ┤ ├ ─ ┼ ─ ┼ ─ ┤  ├ ─ ┼ ─ ┼ ─ ┤ │
   1   0   1     0   1   0      1   0   0   │
 └ ─ ┴ ─ ┴ ─ ┘ └ ─ ┴ ─ ┴ ─ ┘  └ ─ ┴ ─ ┴ ─ ┘ ▼
  ◄──────────────────────────────────────►
    z: As many channel as inputs samples

With those two inputs, we can compute the convolution as such:

        RED                BLUE              GREEN          Output
 ┌ ─ ┬ ─ ┬ ─ ┬───┐  ┌ ─ ┬ ─ ┬ ─ ┬───┐  ┌ ─ ┬ ─ ┬ ─ ┬───┐
   2   2   2   2 │    3   0   0   3 │    4   0   4   4 │
 ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤   ┌───┬───┐
   2   0   0   2 │    0   3   3   0 │    0   4   0   4 │   │ 0 │ 0 │
 ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤ = ├───┼───┤
   2   0   0   2 │    0   3   3   0 │    4   0   4   0 │   │ 0 │ 0 │
 ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤   └───┴───┘
 │ 2 │ 2 │ 2 │ 2 │  │ 3 │ 0 │ 0 │ 3 │  │ 4 │ 4 │ 0 │ 4 │
 └───┴───┴───┴───┘  └───┴───┴───┴───┘  └───┴───┴───┴───┘

So here output value (0,0) is obtained with:

        RED                BLUE              GREEN                      
 ┌ ─ ┬ ─ ┬ ─ ┬───┐  ┌ ─ ┬ ─ ┬ ─ ┬───┐  ┌ ─ ┬ ─ ┬ ─ ┬───┐     ┌─────────┐
  2x1 2x0 2x1  2 │   3x0 0x1 0x0  3 │   4x1 0x1 4x1  4 │     │         │
 ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤   ┌─▼─┬───┐   │
  2x0 0x1 0x0  2 │   0x1 3x0 3x1  0 │   0x1 4x0 0x0  4 │   │24 │ 0 │   │
 ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤ = ├───┼───┤   │
  2x1 0x0 0x1  2 │   0x0 3x1 3x0  0 │   4x1 0x0 4x0  0 │   │ 0 │ 0 │   │
 ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤  ├ ─ ┼ ─ ┼ ─ ┼───┤   └───┴───┘   │
 │ 2 │ 2 │ 2 │ 2 │  │ 3 │ 0 │ 0 │ 3 │  │ 4 │ 4 │ 0 │ 4 │               │
 └───┴───┴───┴───┘  └───┴───┴───┴───┘  └───┴───┴───┴───┘               │
  ─► 2+2+2 = 6       ─► 3+3 = 6         ─► 4+4+4 = 12    = 6+6+12 = 24 ┘                                      

Etc.

         RED                BLUE              GREEN
 ┌───┬ ─ ┬ ─ ┬ ─ ┐  ┌───┬ ─ ┬ ─ ┬ ─ ┐  ┌───┬ ─ ┬ ─ ┬ ─ ┐
 │ 2   2   2   2    │ 3 │ 0   0   3    │ 4 │ 0   4   4  
 ├───┼ ─ ┼ ─ ┼ ─ ┤  ├───┼ ─ ┼ ─ ┼ ─ ┤  ├───┼ ─ ┼ ─ ┼───┤   ┌───┬───┐
 │ 2   0   0   2    │ 0 │ 3   3   0    │ 0 │ 4   0   4     │24 │24 │
 ├───┼ ─ ┼ ─ ┼ ─ ┤  ├───┼ ─ ┼ ─ ┼ ─ ┤  ├───┼ ─ ┼ ─ ┼ ─ ┤ = ├───┼───┤
 │ 2   0   0   2    │ 0 │ 3   3   0    │ 4 │ 0   4   0     │ 0 │ 0 │
 ├───┼ ─ ┼ ─ ┼ ─ ┤  ├───┼ ─ ┼ ─ ┼ ─ ┤  ├───┼ ─ ┼ ─ ┼ ─ ┤   └───┴───┘
 │ 2 │ 2 │ 2 │ 2 │  │ 3 │ 0 │ 0 │ 3 │  │ 4 │ 4 │ 0 │ 4 │
 └───┴───┴───┴───┘  └───┴───┴───┴───┘  └───┴───┴───┴───┘ 

In the end we obtain:

 ┌───┬───┐
 │24 │24 │
 ├───┼───┤
 │24 │24 │
 └───┴───┘

Then we add the biases. Here as we only have one feature map, this is a scalar value that will be
additionned to each value of the output.

### Single sample + multiple feature maps

It is also possible to generate multiple feature maps per sample. In this case, the input stays the
same, however we now have multiple filters:

                               x: Filter size
                               ◄─────────►
 ┌ ─ ┬ ─ ┬ ─ ┐ ┌ ─ ┬ ─ ┬ ─ ┐  ┌ ─ ┬ ─ ┬ ─ ┐ ▲              ▲
   1   0   1     0   1   0      1   1   1   │              │
 ├ ─ ┼ ─ ┼ ─ ┤ ├ ─ ┼ ─ ┼ ─ ┤  ├ ─ ┼ ─ ┼ ─ ┤ │ y:           │
   0   1   0     1   0   1      1   0   0   │ Filter size  │
 ├ ─ ┼ ─ ┼ ─ ┤ ├ ─ ┼ ─ ┼ ─ ┤  ├ ─ ┼ ─ ┼ ─ ┤ │              │
   1   0   1     0   1   0      1   0   0   │              │
 └ ─ ┴ ─ ┴ ─ ┘ └ ─ ┴ ─ ┴ ─ ┘  └ ─ ┴ ─ ┴ ─ ┘ ▼              │ t: Num filters
 ┌ ─ ┬ ─ ┬ ─ ┐ ┌ ─ ┬ ─ ┬ ─ ┐  ┌ ─ ┬ ─ ┬ ─ ┐ ▲              │
   2   0   2     0   2   0      2   2   2   │              │
 ├ ─ ┼ ─ ┼ ─ ┤ ├ ─ ┼ ─ ┼ ─ ┤  ├ ─ ┼ ─ ┼ ─ ┤ │ y:           │
   0   2   0     2   0   2      2   0   0   │ Filter size  │
 ├ ─ ┼ ─ ┼ ─ ┤ ├ ─ ┼ ─ ┼ ─ ┤  ├ ─ ┼ ─ ┼ ─ ┤ │              │
   2   0   2     0   2   0      2   0   0   │              │
 └ ─ ┴ ─ ┴ ─ ┘ └ ─ ┴ ─ ┴ ─ ┘  └ ─ ┴ ─ ┴ ─ ┘ ▼              ▼
  ◄──────────────────────────────────────►
    z: As many channel as inputs samples

For the algorithm, we simply call the previous one for each filter/feature map. We obtain an output
of the shape:

 x: Input.x - filter size + 1
           ◄───────►
 ┌───┬───┐ ┌───┬───┐ ▲
 │24 │24 │ │48 │48 │ │
 ├───┼───┤ ├───┼───┤ │ y: Input.y - filter size + 1
 │24 │24 │ │48 │48 │ │
 └───┴───┘ └───┴───┘ ▼
 ◄─────────────────►
   z: Num filters

### Batch version

For the batch version this is the same principle, we call the convolution function for each sample
for each filter (Filters do not have a batch dimension).

Input batch:

        RED                BLUE              GREEN         
 ┌───┬───┬───┬───┐  ┌───┬───┬───┬───┐  ┌───┬───┬───┬───┐  ▲
 │ 2 │ 2 │ 2 │ 2 │  │ 3 │ 0 │ 0 │ 3 │  │ 4 │ 0 │ 4 │ 4 │  │
 ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  │
 │ 2 │ 0 │ 0 │ 2 │  │ 0 │ 3 │ 3 │ 0 │  │ 0 │ 4 │ 0 │ 4 │  │
 ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  │
 │ 2 │ 0 │ 0 │ 2 │  │ 0 │ 3 │ 3 │ 0 │  │ 4 │ 0 │ 4 │ 0 │  │
 ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  │
 │ 2 │ 2 │ 2 │ 2 │  │ 3 │ 0 │ 0 │ 3 │  │ 4 │ 4 │ 0 │ 4 │  │
 └───┴───┴───┴───┘  └───┴───┴───┴───┘  └───┴───┴───┴───┘  │
                                                          │ Batch size
        RED                BLUE              GREEN        │
 ┌───┬───┬───┬───┐  ┌───┬───┬───┬───┐  ┌───┬───┬───┬───┐  │
 │ 2 │ 2 │ 2 │ 2 │  │ 3 │ 0 │ 0 │ 3 │  │ 4 │ 0 │ 4 │ 4 │  │
 ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  │
 │ 2 │ 0 │ 0 │ 2 │  │ 0 │ 3 │ 3 │ 0 │  │ 0 │ 4 │ 0 │ 4 │  │
 ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  │
 │ 2 │ 0 │ 0 │ 2 │  │ 0 │ 3 │ 3 │ 0 │  │ 4 │ 0 │ 4 │ 0 │  │
 ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  ├───┼───┼───┼───┤  │
 │ 2 │ 2 │ 2 │ 2 │  │ 3 │ 0 │ 0 │ 3 │  │ 4 │ 4 │ 0 │ 4 │  │
 └───┴───┴───┴───┘  └───┴───┴───┴───┘  └───┴───┴───┴───┘  ▼

We obtain an output of the shape:

         x: Input.x - filter size + 1
                   ◄───────►
       ▲ ┌───┬───┐ ┌───┬───┐ ▲
       │ │24 │24 │ │48 │48 │ │
       │ ├───┼───┤ ├───┼───┤ │ y: Input.y - filter size + 1
       │ │24 │24 │ │48 │48 │ │
 t:    │ └───┴───┘ └───┴───┘ ▼
 Batch │ ┌───┬───┐ ┌───┬───┐ 
 size  │ │24 │24 │ │48 │48 │ 
       │ ├───┼───┤ ├───┼───┤ 
       │ │24 │24 │ │48 │48 │ 
       ▼ └───┴───┘ └───┴───┘ 
         ◄─────────────────► 
           z: Num filters     

Here the biases are stored in a vector with size `num filters`, each scalar value will be added in
the corresponding filter for each batch:

Biases:
 ┌───┬───┐
 │-1 │ 1 │
 └───┴───┘

Output:
 ┌───┬───┐ ┌───┬───┐
 │23 │23 │ │49 │49 │
 ├───┼───┤ ├───┼───┤
 │23 │23 │ │49 │49 │
 └───┴───┘ └───┴───┘
 ┌───┬───┐ ┌───┬───┐
 │23 │23 │ │49 │49 │
 ├───┼───┤ ├───┼───┤
 │23 │23 │ │49 │49 │
 └───┴───┘ └───┴───┘

## Forward relu

(I changed the values to introduces negatives)
Nothing crazy here:

Input:
 ┌───┬───┐ ┌───┬───┐
 │-1 │ 2 │ │-5 │-5 │
 ├───┼───┤ ├───┼───┤
 │-9 │ 5 │ │-2 │-3 │
 └───┴───┘ └───┴───┘
 ┌───┬───┐ ┌───┬───┐
 │ 9 │-2 │ │ 0 │ 4 │
 ├───┼───┤ ├───┼───┤
 │ 0 │ 0 │ │-3 │ 7 │
 └───┴───┘ └───┴───┘

Output:
 ┌───┬───┐ ┌───┬───┐
 │ 0 │ 2 │ │ 0 │ 0 │
 ├───┼───┤ ├───┼───┤
 │ 0 │ 5 │ │ 0 │ 0 │
 └───┴───┘ └───┴───┘
 ┌───┬───┐ ┌───┬───┐
 │ 9 │ 0 │ │ 0 │ 4 │
 ├───┼───┤ ├───┼───┤
 │ 0 │ 0 │ │ 0 │ 7 │
 └───┴───┘ └───┴───┘

## Forward max pooling

### Single sample with one feature map

Here we take another input example because the previous one (output of the forward) is too small
Input:

 ┌───┬───┬───┬───┐  
 │ 6 │ 2 │ 8 │ 5 │  
 ├───┼───┼───┼───┤  
 │ 7 │ 1 │ 7 │ 5 │  
 ├───┼───┼───┼───┤  
 │ 1 │ 2 │ 1 │ 8 │  
 ├───┼───┼───┼───┤  
 │ 8 │ 2 │ 7 │ 9 │  
 └───┴───┴───┴───┘  

Algorithm:

       Input                  Input                  Input                  Input
 ┌ ─ ┬ ─ ┬───┬───┐      ┌───┬───┬ ─ ┬ ─ ┐      ┌───┬───┬───┬───┐      ┌───┬───┬───┬───┐
   6   2   8 │ 5 │      │ 6 │ 2   8   5        │ 6 │ 2 │ 8 │ 5 │      │ 6 │ 2 │ 8 │ 5 │
 ├ ─ ┼ ─ ┼───┼───┤      ├───┼───┼ ─ ┼ ─ ┤      ├───┼───┼───┼───┤      ├───┼───┼───┼───┤
   7   1   7 │ 5 │      │ 7 │ 1   7   5        │ 7 │ 1 │ 7 │ 5 │      │ 7 │ 1 │ 7 │ 5 │
 ├ ─ ┼ ─ ┼───┼───┤ ───► ├───┼───┼ ─ ┼ ─ ┤ ───► ├ ─ ┼ ─ ┼───┼───┤ ───► ├───┼───┼ ─ ┼ ─ ┤
 │ 1 │ 2 │ 1 │ 8 │      │ 1 │ 2 │ 1 │ 8 │        1   2   1 │ 8 │      │ 1 │ 2   1   8  
 ├───┼───┼───┼───┤      ├───┼───┼───┼───┤      ├ ─ ┼ ─ ┼───┼───┤      ├───┼───┼ ─ ┼ ─ ┤
 │ 8 │ 2 │ 7 │ 9 │      │ 8 │ 2 │ 7 │ 9 │        8   2   7 │ 9 │      │ 8 │ 2   7   9  
 └───┴───┴───┴───┘      └───┴───┴───┴───┘      └ ─ ┴ ─ ┴───┴───┘      └───┴───┴ ─ ┴ ─ ┘
                                                                                   
       Ouput                  Ouput                  Ouput                  Ouput
     ┌───┬───┐              ┌───┬───┐              ┌───┬───┐              ┌───┬───┐
     │ 7 │ 0 │              │ 7 │ 8 │              │ 7 │ 8 │              │ 7 │ 8 │
     ├───┼───┤     ───►     ├───┼───┤     ───►     ├───┼───┤     ───►     ├───┼───┤
     │ 0 │ 0 │              │ 0 │ 0 │              │ 8 │ 0 │              │ 8 │ 9 │
     └───┴───┘              └───┴───┘              └───┴───┘              └───┴───┘

       Mask                   Mask                   Mask                   Mask
 ┌───┬───┬───┬───┐      ┌───┬───┬───┬───┐      ┌───┬───┬───┬───┐      ┌───┬───┬───┬───┐ 
 │ 0 │ 0 │ 0 │ 0 │      │ 0 │ 0 │ 1 │ 0 │      │ 0 │ 0 │ 1 │ 0 │      │ 0 │ 0 │ 1 │ 0 │ 
 ├───┼───┼───┼───┤      ├───┼───┼───┼───┤      ├───┼───┼───┼───┤      ├───┼───┼───┼───┤ 
 │ 1 │ 0 │ 0 │ 0 │      │ 1 │ 0 │ 0 │ 0 │      │ 1 │ 0 │ 0 │ 0 │      │ 1 │ 0 │ 0 │ 0 │ 
 ├───┼───┼───┼───┤ ───► ├───┼───┼───┼───┤ ───► ├───┼───┼───┼───┤ ───► ├───┼───┼───┼───┤ 
 │ 0 │ 0 │ 0 │ 0 │      │ 0 │ 0 │ 0 │ 0 │      │ 0 │ 0 │ 0 │ 0 │      │ 0 │ 0 │ 0 │ 0 │ 
 ├───┼───┼───┼───┤      ├───┼───┼───┼───┤      ├───┼───┼───┼───┤      ├───┼───┼───┼───┤ 
 │ 0 │ 0 │ 0 │ 0 │      │ 0 │ 0 │ 0 │ 0 │      │ 1 │ 0 │ 0 │ 0 │      │ 1 │ 0 │ 0 │ 1 │ 
 └───┴───┴───┴───┘      └───┴───┴───┴───┘      └───┴───┴───┴───┘      └───┴───┴───┴───┘ 

The algorithm is fairly straigthforward, however notice that we store the indices of max values into
another buffer called mask.
This will be useful for the backward pass as we will only propagate gradients where the max values
were.

### Batch version + multiple feature maps

Max pooling is performed per feature map, per sample.
So in the end we may obtain something such as:

Input:
                              x: Input.x
                           ◄───────────────►
       ▲ ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐ ▲                                  
       │ │ 6 │ 2 │ 8 │ 5 │ │ 2 │ 8 │ 7 │ 6 │ │                                  
       │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ │                                  
       │ │ 7 │ 1 │ 7 │ 5 │ │ 6 │ 8 │ 5 │ 0 │ │                                  
       │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ │ y: Input.y                                 
       │ │ 1 │ 2 │ 1 │ 8 │ │ 2 │ 7 │ 7 │ 8 │ │                                  
       │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ │                                  
 t:    │ │ 8 │ 2 │ 7 │ 9 │ │ 3 │ 7 │ 6 │ 8 │ │                                  
 Batch │ └───┴───┴───┴───┘ └───┴───┴───┴───┘ ▼                                  
 size  │ ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
       │ │ 5 │ 9 │ 0 │ 5 │ │ 9 │ 9 │ 5 │ 3 │
       │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
       │ │ 9 │ 0 │ 0 │ 2 │ │ 8 │ 3 │ 3 │ 9 │
       │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
       │ │ 7 │ 7 │ 6 │ 2 │ │ 8 │ 4 │ 4 │ 8 │
       │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
       │ │ 5 │ 7 │ 9 │ 5 │ │ 4 │ 7 │ 5 │ 8 │
       ▼ └───┴───┴───┴───┘ └───┴───┴───┴───┘
         ◄─────────────────────────────────►
                   z: Num filters


Ouput:
                x: Input.x / 2                       
                   ◄───────►                                
       ▲ ┌───┬───┐ ┌───┬───┐ ▲                              
       │ │ 7 │ 8 │ │ 8 │ 7 │ │                              
       │ ├───┼───┤ ├───┼───┤ │ y: Input.y / 2
       │ │ 8 │ 9 │ │ 7 │ 8 │ │                              
 t:    │ └───┴───┘ └───┴───┘ ▼                              
 Batch │ ┌───┬───┐ ┌───┬───┐                                
 size  │ │ 9 │ 5 │ │ 9 │ 9 │                                
       │ ├───┼───┤ ├───┼───┤                                
       │ │ 7 │ 9 │ │ 8 │ 8 │                                
       ▼ └───┴───┘ └───┴───┘                                
         ◄─────────────────►                                
           z: Num filters                                   

Masks (same shape as input):

 ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
 │ 0 │ 0 │ 1 │ 0 │ │ 0 │ 0 │ 1 │ 0 │ Note: here when we have multiple max values that are equal
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ we take the index of the last one in the window.
 │ 1 │ 0 │ 0 │ 0 │ │ 0 │ 1 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 1 │ 0 │ 0 │ 1 │ │ 0 │ 1 │ 0 │ 1 │
 └───┴───┴───┴───┘ └───┴───┴───┴───┘
 ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
 │ 0 │ 0 │ 0 │ 1 │ │ 0 │ 1 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 1 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 1 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 0 │ 0 │ 0 │ │ 1 │ 0 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 1 │ 1 │ 0 │ │ 0 │ 0 │ 0 │ 1 │
 └───┴───┴───┴───┘ └───┴───┴───┴───┘

## Forward flatten

Here we simply flatten every 3 dimensionnal arrays over the batch size dimension.
So if we keep the previous values:

Input:
       ▲ ┌───┬───┐ ┌───┬───┐
       │ │ 7 │ 8 │ │ 8 │ 7 │
       │ ├───┼───┤ ├───┼───┤
       │ │ 8 │ 9 │ │ 7 │ 8 │
 t:    │ └───┴───┘ └───┴───┘
 Batch │ ┌───┬───┐ ┌───┬───┐
 size  │ │ 9 │ 5 │ │ 9 │ 9 │
       │ ├───┼───┤ ├───┼───┤
       │ │ 7 │ 9 │ │ 8 │ 8 │
       ▼ └───┴───┘ └───┴───┘

Ouput:
       ▲ ┌───┬───┬───┬───┬───┬───┬───┬───┐
 y:    │ │ 7 │ 8 │ 8 │ 9 │ 8 │ 7 │ 7 │ 8 │
 Batch │ └───┴───┴───┴───┴───┴───┴───┴───┘
 size  │ ┌───┬───┬───┬───┬───┬───┬───┬───┐
       │ │ 9 │ 5 │ 7 │ 9 │ 9 │ 9 │ 8 │ 8 │
       ▼ └───┴───┴───┴───┴───┴───┴───┴───┘
         ◄───────────────────────────────►
          x: Input.x * Input.y * Input.z

## Forward dense

Like in the convolution layer we have two types of learnable parameters:

Weights:
             x: Input.x
 ◄───────────────────────────────►
 ┌───┬───┬───┬───┬───┬───┬───┬───┐ ▲
 │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ 3 │ 2 │ │
 ├───┼───┼───┼───┼───┼───┼───┼───┤ │
 │ 2 │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ 3 │ │ y: Num classes
 ├───┼───┼───┼───┼───┼───┼───┼───┤ │
 │ 3 │ 2 │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ │
 └───┴───┴───┴───┴───┴───┴───┴───┘ ▼

Biases:
 ┌───┬───┬───┐
 │-1 │ 1 │ 2 │
 └───┴───┴───┘
 ◄───────────►
 x: Num classes

### Single sample

So let's use the output from the flatten at the first index of the batch size. Here we perform a 
matrix vector product between the weights and the layer's input.

                                   ┌───┐ ▲
                                   │ 7 │ │
                                   ├───┤ │
                                   │ 8 │ │
                                   ├───┤ │
                                   │ 8 │ │
                                   ├───┤ │
                                   │ 9 │ │
                             Input ├───┤ │ x: Input.x
                                   │ 8 │ │
                                   ├───┤ │
                                   │ 7 │ │
                                   ├───┤ │
                                   │ 7 │ │                          
                                   ├───┤ │                          
                                   │ 8 │ │                          
              Weights              └───┘ ▼                          
 ┌───┬───┬───┬───┬───┬───┬───┬───┐ ┌───┐ ▲
 │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ 3 │ 2 │ │124│ │
 ├───┼───┼───┼───┼───┼───┼───┼───┤ ├───┤ │
 │ 2 │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ 3 │ │126│ │ y: Num classes
 ├───┼───┼───┼───┼───┼───┼───┼───┤ ├───┤ │
 │ 3 │ 2 │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ │124│ │
 └───┴───┴───┴───┴───┴───┴───┴───┘ └───┘ ▼
 ◄───────────────────────────────► 
            x: Input.x
                                        
Then we add biases:

 ┌───┬───┬───┐   ┌───┬───┬───┐   ┌───┬───┬───┐
 │124│126│122│ + │-1 │ 1 │ 2 │ = │123│127│126│
 └───┴───┴───┘   └───┴───┴───┘   └───┴───┴───┘

And we obtain the predictions for this current sample!

### Batch version

Same principle but instead we just use a matrix matrix multiplication.

                                    ┌───┬───┐
                                    │ 7 │ 9 │
                                    ├───┼───┤
                                    │ 8 │ 5 │
                                    ├───┼───┤
                                    │ 8 │ 7 │
                                    ├───┼───┤
                                    │ 9 │ 9 │
                              Input ├───┼───┤
                                    │ 8 │ 9 │
                                    ├───┼───┤
                                    │ 7 │ 9 │
                                    ├───┼───┤
                                    │ 7 │ 8 │
                                    ├───┼───┤
                                    │ 8 │ 8 │
               Weights              └───┴───┘
  ┌───┬───┬───┬───┬───┬───┬───┬───┐ ┌───┬───┐ ▲
  │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ 3 │ 2 │ │124│125│ │
  ├───┼───┼───┼───┼───┼───┼───┼───┤ ├───┼───┤ │
  │ 2 │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ 3 │ │126│131│ │ x: Num classes
  ├───┼───┼───┼───┼───┼───┼───┼───┤ ├───┼───┤ │
  │ 3 │ 2 │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ │124│131│ │
  └───┴───┴───┴───┴───┴───┴───┴───┘ └───┴───┘ ▼
                                    ◄───────►
                                  y: Batch size
Then we add biases:

 ┌───┬───┬───┐   ┌───┬───┬───┐   ┌───┬───┬───┐
 │124│126│124│   │-1 │ 1 │ 2 │   │123│127│126│
 ├───┼───┼───┤ + ├───┼───┼───┤ = ├───┼───┼───┤
 │125│131│131│   │-1 │ 1 │ 2 │   │124│132│133│
 └───┴───┴───┘   └───┴───┴───┘   └───┴───┴───┘

Note: In DAHL we actually perform multiple matrix vector operations instead of doing one matrix
matrix multiplication for the whole batch. I am unsure which one is best for now. The advantage is
that we don't need to transpose the inputs are they are taken as vectors.

## Loss, checking predictions and gradients

TODO

## Backward dense

Here we need to compute three things: 
1. the derivative of weights with respect to gradients -> The output of the backward pass
2. the derivative of the layer's forward input with respect to gradients -> It will help us updating
   our weights
3. the sum of the gradients per batch -> In order to update the biases

### Computing the output

Here we compte the partial result for each sample with a vector matrix product, so that the
resulting shape fits the forward input shape.

                       x: forward_input.x 
               ◄───────────────────────────────►
               ┌───┬───┬───┬───┬───┬───┬───┬───┐ ▲
               │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ 3 │ 2 │ │
               ├───┼───┼───┼───┼───┼───┼───┼───┤ │
       Weights │ 2 │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ 3 │ │ y: Num classes
               ├───┼───┼───┼───┼───┼───┼───┼───┤ │
               │ 3 │ 2 │ 1 │ 2 │ 3 │ 2 │ 1 │ 2 │ │
   Gradients   └───┴───┴───┴───┴───┴───┴───┴───┘ ▼
 ┌───┬───┬───┐ ┌───┬───┬───┬───┬───┬───┬───┬───┐
 │123│127│126│ │   │   │   │   │   │   │   │   │
 └───┴───┴───┘ └───┴───┴───┴───┴───┴───┴───┴───┘
 ◄───────────►
 x: Num classes

Then it gets repeated for each sample so that we obtain a result matrix as such:
 
       ▲ ┌───┬───┬───┬───┬───┬───┬───┬───┐
 y:    │ │...│...│...│...│...│...│...│...│
 Batch │ ├───┼───┼───┼───┼───┼───┼───┼───┤
 size  │ │...│...│...│...│...│...│...│...│
       ▼ └───┴───┴───┴───┴───┴───┴───┴───┘
         ◄───────────────────────────────►
                 x: forward_input.x

### Updating weights
                   
First compute dl_dw with an outer product between gradients and forward input:

                           forward input                                 
                 ┌───┬───┬───┬───┬───┬───┬───┬───┐                       
                 │ 7 │ 8 │ 8 │ 9 │ 8 │ 7 │ 7 │ 8 │                       
                 └───┴───┴───┴───┴───┴───┴───┴───┘                                                   
           ┌───┐ ┌───┬───┬───┬───┬───┬───┬───┬───┐ ▲                                                 
           │123│ │...│...│...│...│...│...│...│...│ │                                                 
           ├───┤ ├───┼───┼───┼───┼───┼───┼───┼───┤ │                                                 
 Gradients │127│ │...│...│...│...│...│...│...│...│ │ y: Num classes                                  
           ├───┤ ├───┼───┼───┼───┼───┼───┼───┼───┤ │                                                 
           │126│ │...│...│...│...│...│...│...│...│ │                                                 
           └───┘ └───┴───┴───┴───┴───┴───┴───┴───┘ ▼                                                 
                 ◄───────────────────────────────►                                                  
                       x: forward_input.x                             

Then we simply multiply dl_dw by the learning rate and add this result to weights.


-> for the batch version we do that for each pair of forward input and gradients.
The result is summed up in dl_dw, scaled by learning rate then incremented into the weights.

### Updating biases

Here its pretty easy, we just have to sum each gradients over the batch dimension and increment it to the biases
vector.
                       
             Gradients         ┌───┬───┬───┐  
         ▲ ┌───┬───┬───┐       │123│127│126│       dl_db
         │ │123│127│126│       └───┴───┴───┘   ┌───┬───┬───┐ 
y: Batch │ ├───┼───┼───┤ ───►        +       = │247│259│259│                                                         
   size  │ │124│132│133│       ┌───┬───┬───┐   └───┴───┴───┘                                                       
         ▼ └───┴───┴───┘       │124│132│133│                                             
           ◄───────────►       └───┴───┴───┘                   
           x: Num classes       
                    
           
Like for weights, we scale dl_db by the learning rate then increment the biases.

## Backward flatten (unflatten)

Here pretty straight-forward, same principle than forward but in reverse

Input:

       ▲ ┌───┬───┬───┬───┬───┬───┬───┬───┐
 y:    │ │...│...│...│...│...│...│...│...│
 Batch │ ├───┼───┼───┼───┼───┼───┼───┼───┤
 size  │ │...│...│...│...│...│...│...│...│
       ▼ └───┴───┴───┴───┴───┴───┴───┴───┘
         ◄───────────────────────────────►
                 x: forward_input.x

 Ouput:

       ▲ ┌───┬───┐ ┌───┬───┐
       │ │...│...│ │...│...│
       │ ├───┼───┤ ├───┼───┤
       │ │...│...│ │...│...│
 t:    │ └───┴───┘ └───┴───┘
 Batch │ ┌───┬───┐ ┌───┬───┐
 size  │ │...│...│ │...│...│
       │ ├───┼───┤ ├───┼───┤
       │ │...│...│ │...│...│
       ▼ └───┴───┘ └───┴───┘

## Backward max pooling

Now, remember that last max pooling forward, we kept the max indexes in `masks`:

                              x: forward_input.x
                              ◄───────────────►
          ▲ ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐ ▲                                                    
          │ │ 0 │ 0 │ 1 │ 0 │ │ 0 │ 0 │ 1 │ 0 │ │                                                    
          │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ │                                                    
          │ │ 1 │ 0 │ 0 │ 0 │ │ 0 │ 1 │ 0 │ 0 │ │                                                    
          │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ │ y: forward_input.y                                                    
          │ │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ │                                                    
          │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ │                                                    
          │ │ 1 │ 0 │ 0 │ 1 │ │ 0 │ 1 │ 0 │ 1 │ │                                                    
          │ └───┴───┴───┴───┘ └───┴───┴───┴───┘ ▼                                                    
 t: Batch │ ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐                                                      
    size  │ │ 0 │ 0 │ 0 │ 1 │ │ 0 │ 1 │ 0 │ 0 │                                                      
          │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤                                                      
          │ │ 1 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 1 │                                                      
          │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤                                                      
          │ │ 0 │ 0 │ 0 │ 0 │ │ 1 │ 0 │ 0 │ 0 │                                                      
          │ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤                                                      
          │ │ 0 │ 1 │ 1 │ 0 │ │ 0 │ 0 │ 0 │ 1 │                                                      
          ▼ └───┴───┴───┴───┘ └───┴───┴───┴───┘                                                      
            ◄─────────────────────────────────►                                                      
                      z: Num filters                                                                 

Thanks to the mask, we will be able to propagate the gradient values only on the indexes that were
"selected" during the forward pass.
(Here I changed Input values so we can see where the results gets propagated)

       ┌───────────────────────────┐
       │                 ┌───┬───┬─▼─┬───┐ ┌───┬───┬───┬───┐   ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
       │                 │ 0 │ 0 │ 1 │ 0 │ │ 0 │ 0 │ 1 │ 0 │   │ 0 │ 0 │ 2 │ 0 │ │ 0 │ 0 │ 5 │ 0 │
       │                 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
   ┌───┼─────────────────► 1 │ 0 │ 0 │ 0 │ │ 0 │ 1 │ 0 │ 0 │   │ 8 │ 0 │ 0 │ 0 │ │ 0 │ 7 │ 0 │ 0 │
 ┌─┴─┬─┴─┐   ┌───┬───┐   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 8 │ 2 │   │ 7 │ 5 │   │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │   │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │
 ├───┼───┤   ├───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 5 │ 1 │   │ 4 │ 7 │   │ 1 │ 0 │ 0 │ 1 │ │ 0 │ 1 │ 0 │ 1 │   │ 5 │ 0 │ 0 │ 1 │ │ 0 │ 4 │ 0 │ 7 │
 └─┬─┴─┬─┘   └───┴───┘   └─▲─┴───┴───┴─▲─┘ └───┴───┴───┴───┘   └───┴───┴───┴───┘ └───┴───┴───┴───┘
   └───┴───────────────x───┴───────────┘                     =
 ┌───┬───┐   ┌───┬───┐   ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐   ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
 │ 5 │ 1 │   │ 4 │ 4 │   │ 0 │ 0 │ 0 │ 1 │ │ 0 │ 1 │ 0 │ 0 │   │ 0 │ 0 │ 0 │ 1 │ │ 0 │ 4 │ 0 │ 0 │
 ├───┼───┤   ├───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 9 │ 6 │   │ 0 │ 3 │   │ 1 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 1 │   │ 5 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 4 │
 └───┴───┘   └───┴───┘   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
         Input           │ 0 │ 0 │ 0 │ 0 │ │ 1 │ 0 │ 0 │ 0 │   │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │
                         ├───┼───┼───┼───┤ ├───┼───┼───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
                         │ 0 │ 1 │ 1 │ 0 │ │ 0 │ 0 │ 0 │ 1 │   │ 0 │ 9 │ 6 │ 0 │ │ 0 │ 0 │ 0 │ 3 │
                         └───┴───┴───┴───┘ └───┴───┴───┴───┘   └───┴───┴───┴───┘ └───┴───┴───┴───┘
                                        Masks                                 Output

## Backward relu

Here we will only propagate gradients where the previous input value (during the forward pass) were
positive. We could clearly use a mask like we did in the pooling layer.

Input:
 ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
 │ 0 │ 0 │ 2 │ 0 │ │ 0 │ 0 │ 5 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 8 │ 0 │ 0 │ 0 │ │ 0 │ 7 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 5 │ 0 │ 0 │ 1 │ │ 0 │ 4 │ 0 │ 7 │
 └───┴───┴───┴───┘ └───┴───┴───┴───┘
 ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
 │ 0 │ 0 │ 0 │ 1 │ │ 0 │ 4 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 5 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 4 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 9 │ 6 │ 0 │ │ 0 │ 0 │ 0 │ 3 │
 └───┴───┴───┴───┘ └───┴───┴───┴───┘

Relu forward input (example):
 ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
 │-1 │ 5 │ 2 │ 3 │ │ 5 │ 8 │ 9 │ 9 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │-7 │ 9 │ 4 │ 7 │ │ 4 │-1 │ 6 │ 4 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 6 │-6 │ 4 │ 8 │ │ 6 │ 0 │ 5 │-4 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │-9 │ 5 │ 2 │ 2 │ │ 3 │ 7 │ 6 │ 9 │
 └───┴───┴───┴───┘ └───┴───┴───┴───┘
 ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
 │ 5 │ 8 │ 9 │ 9 │ │ 5 │ 1 │ 7 │-1 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │-3 │ 7 │ 3 │ 7 │ │ 1 │-8 │ 6 │ 9 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 5 │ 0 │ 0 │ │ 7 │ 5 │ 4 │ 7 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 3 │ 6 │ 3 │ 6 │ │ 4 │-1 │ 9 │ 4 │
 └───┴───┴───┴───┘ └───┴───┴───┴───┘

Output:
 ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
 │ 0 │ 0 │ 2 │ 0 │ │ 0 │ 0 │ 5 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 0 │ 0 │ 1 │ │ 0 │ 4 │ 0 │ 7 │
 └───┴───┴───┴───┘ └───┴───┴───┴───┘
 ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
 │ 0 │ 0 │ 0 │ 1 │ │ 0 │ 4 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 4 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 9 │ 6 │ 0 │ │ 0 │ 0 │ 0 │ 3 │
 └───┴───┴───┴───┘ └───┴───┴───┴───┘

 ## Backward convolution

Like in the bacward dense, we need to compute the three same things:
1. the derivative of filters with respect to gradients -> The output of the backward pass
2. the derivative of the layer's forward input with respect to gradients -> It will help us updating
   our weights
3. the sum of the gradients per batch -> In order to update the biases 

### Computing the output

Note that this part is only required when the convolution is not the first layer in the network.
However there is no point computing the output because we have no layer to propagate it to.

Here we need the gradients (dl_dout) and the filters. First we need to add padding to the gradients
because we want to end up with a larger output as we are going backwards.

(taking dl_dout with a smaller dimension to imitate the forward pass)

Padding to be added on each dimension is equals to `(filter size - 1) * 2`, here (3 - 1) * 2.

                  dl_dout
                      x: dl_dout.x
                      ◄───────►
          ▲ ┌───┬───┐ ┌───┬───┐ ▲        
          │ │ 5 │ 0 │ │ 7 │ 0 │ │
          │ ├───┼───┤ ├───┼───┤ │ y: dl_dout.y
          │ │ 0 │ 1 │ │ 0 │ 0 │ │
 t: Batch │ └───┴───┘ └───┴───┘ ▼
    size  │ ┌───┬───┐ ┌───┬───┐
          │ │ 1 │ 0 │ │ 0 │ 1 │
          │ ├───┼───┤ ├───┼───┤
          │ │ 8 │ 0 │ │ 0 │ 6 │
          ▼ └───┴───┘ └───┴───┘
            ◄─────────────────►
              z: Num filters

We obtain dl_dout_padded:
                                  x: dl_dout.x + (filter size - 1) * 2
                                      ◄───────────────────────►
          ▲ ┌───┬───┬───┬───┬───┬───┐ ┌───┬───┬───┬───┬───┬───┐ ▲
          │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │
          │ ├───┼───┼───┼───┼───┼───┤ ├───┼───┼───┼───┼───┼───┤ │
          │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │
          │ ├───┼───┼───┼───┼───┼───┤ ├───┼───┼───┼───┼───┼───┤ │
          │ │ 0 │ 0 │ 5 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 7 │ 0 │ 0 │ 0 │ │
          │ ├───┼───┼───┼───┼───┼───┤ ├───┼───┼───┼───┼───┼───┤ │ y: dl_dout.y + (filter size - 1) * 2
          │ │ 0 │ 0 │ 0 │ 1 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │
          │ ├───┼───┼───┼───┼───┼───┤ ├───┼───┼───┼───┼───┼───┤ │
          │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │
          │ ├───┼───┼───┼───┼───┼───┤ ├───┼───┼───┼───┼───┼───┤ │
          │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │
 t: Batch │ └───┴───┴───┴───┴───┴───┘ └───┴───┴───┴───┴───┴───┘ ▼
    size  │ ┌───┬───┬───┬───┬───┬───┐ ┌───┬───┬───┬───┬───┬───┐
          │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │
          │ ├───┼───┼───┼───┼───┼───┤ ├───┼───┼───┼───┼───┼───┤
          │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │
          │ ├───┼───┼───┼───┼───┼───┤ ├───┼───┼───┼───┼───┼───┤
          │ │ 0 │ 0 │ 1 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 1 │ 0 │ 0 │
          │ ├───┼───┼───┼───┼───┼───┤ ├───┼───┼───┼───┼───┼───┤
          │ │ 0 │ 0 │ 8 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 6 │ 0 │ 0 │
          │ ├───┼───┼───┼───┼───┼───┤ ├───┼───┼───┼───┼───┼───┤
          │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │
          │ ├───┼───┼───┼───┼───┼───┤ ├───┼───┼───┼───┼───┼───┤
          │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │
          ▼ └───┴───┴───┴───┴───┴───┘ └───┴───┴───┴───┴───┴───┘
            ◄─────────────────────────────────────────────────►
                              z: Num filters

Then for each sample for each filter we do a convolution of dl_dout_padded with the filters.
Note: the function takes a matrix (dl_dout), a 3d matrix (filters) and outputs a 3d matrix.
This means that we will be sliding 3 kernels (the filters) on the same input matrix and store each
results in their corresponding channel dimension.
Note 2: the kernel should be rotated by 180 degrees, however in the implementation we only need to
play with the indices, no need to **actually** rotate the matrices.

Filters:
                                  x: Filter size                                                         
                                 ◄─────────►                                                             
   ┌ ─ ┬ ─ ┬ ─ ┐ ┌ ─ ┬ ─ ┬ ─ ┐  ┌ ─ ┬ ─ ┬ ─ ┐ ▲                                                          
     1   0   1     0   1   0      1   1   1   │                                                          
   ├ ─ ┼ ─ ┼ ─ ┤ ├ ─ ┼ ─ ┼ ─ ┤  ├ ─ ┼ ─ ┼ ─ ┤ │                                                          
     0   1   0     1   0   1      1   0   0   │ y: Filter size                                           
   ├ ─ ┼ ─ ┼ ─ ┤ ├ ─ ┼ ─ ┼ ─ ┤  ├ ─ ┼ ─ ┼ ─ ┤ │                                                          
     1   0   1     0   1   0      1   0   0   │                                                          
   └ ─ ┴ ─ ┴ ─ ┘ └ ─ ┴ ─ ┴ ─ ┘  └ ─ ┴ ─ ┴ ─ ┘ ▼                                                          
    ◄──────────────────────────────────────►                                                             
      z: As many channel as inputs samples                                                               
                                                                                                         
Rotated filters (not the best examples):

   ┌ ─ ┬ ─ ┬ ─ ┐ ┌ ─ ┬ ─ ┬ ─ ┐  ┌ ─ ┬ ─ ┬ ─ ┐
     1   0   1     0   1   0      0   0   1  
   ├ ─ ┼ ─ ┼ ─ ┤ ├ ─ ┼ ─ ┼ ─ ┤  ├ ─ ┼ ─ ┼ ─ ┤
     0   1   0     1   0   1      0   0   1  
   ├ ─ ┼ ─ ┼ ─ ┤ ├ ─ ┼ ─ ┼ ─ ┤  ├ ─ ┼ ─ ┼ ─ ┤
     1   0   1     0   1   0      1   1   1  
   └ ─ ┴ ─ ┴ ─ ┘ └ ─ ┴ ─ ┴ ─ ┘  └ ─ ┴ ─ ┴ ─ ┘

Actually do the convolution, here on sample 0, filter 0:
(Here we ignore zeros, but of course they would be multiplied by the corresponding kernel value)

   0+...0+5x(ker val) ─────────┬─────────────────┬─────────────────┐
 ┌ ─ ┬ ─ ┬ ─ ┬───┬───┬───┐     │                 │                 │
   0   0   0   0 │ 0 │ 0 │     │ ker val=1       │ ker val=0       │ ker val=1
 ├ ─ ┼ ─ ┼ ─ ┼───┼───┼───┤   ┌─▼─┬───┬───┬───┐ ┌─▼─┬───┬───┬───┐ ┌─▼─┬───┬───┬───┐ 
   0   0   0   0 │ 0 │ 0 │   │ 5 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ │ 5 │ 0 │ 0 │ 0 │ 
 ├ ─ ┼ ─ ┼ ─ ┼───┼───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ 
   0   0   5   0 │ 0 │ 0 │   │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 
 ├ ─ ┼ ─ ┼ ─ ┼───┼───┼───┤ = ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ 
 │ 0 │ 0 │ 0 │ 1 │ 0 │ 0 │   │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 
 ├───┼───┼───┼───┼───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ 
 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │   │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ 
 ├───┼───┼───┼───┼───┼───┤   └───┴───┴───┴───┘ └───┴───┴───┴───┘ └───┴───┴───┴───┘ 
 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │
 └───┴───┴───┴───┴───┴───┘
                           
   0+...0+5x(ker val)+0 ───────────┬─────────────────┬─────────────────┐
 ┌───┬ ─ ┬ ─ ┬ ─ ┬───┬───┐         │                 │                 │                                                                                     
 │ 0   0   0   0   0 │ 0 │         │ ker val=0       │ ker val=1       │ ker val=1                                                                           
 ├───┼ ─ ┼ ─ ┼ ─ ┼───┼───┤   ┌───┬─▼─┬───┬───┐ ┌───┬─▼─┬───┬───┐ ┌───┬─▼─┬───┬───┐
 │ 0   0   0   0   0 │ 0 │   │ 5 │ 0 │ 0 │ 0 │ │ 0 │ 5 │ 0 │ 0 │ │ 5 │ 5 │ 0 │ 0 │
 ├───┼ ─ ┼ ─ ┼ ─ ┼───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0   0   5   0   0 │ 0 │   │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │
 ├───┼ ─ ┼ ─ ┼ ─ ┼───┼───┤ = ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 0 │ 0 │ 1 │ 0 │ 0 │   │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │
 ├───┼───┼───┼───┼───┼───┤   ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │   │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │
 ├───┼───┼───┼───┼───┼───┤   └───┴───┴───┴───┘ └───┴───┴───┴───┘ └───┴───┴───┴───┘
 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │                     
 └───┴───┴───┴───┴───┴───┘                     

etc.

So we obtain the same shape as our forward input (for example our images if it were the first layer
in the network).

                                     x: forward_input.x
                                     ◄───────────────►
 ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐ ▲
 │ 5 │ 0 │ 5 │ 0 │ │ 0 │ 5 │ 0 │ 0 │ │ 5 │ 5 │ 5 │ 0 │ │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ │
 │ 0 │ 6 │ 0 │ 0 │ │ 5 │ 0 │ 6 │ 0 │ │ 5 │ 1 │ 1 │ 1 │ │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ │ y: forward_input.y
 │ 5 │ 0 │ 6 │ 0 │ │ 0 │ 6 │ 0 │ 1 │ │ 5 │ 1 │ 0 │ 0 │ │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ │
 │ 0 │ 1 │ 0 │ 1 │ │ 0 │ 0 │ 1 │ 0 │ │ 0 │ 1 │ 0 │ 0 │ │
 └───┴───┴───┴───┘ └───┴───┴───┴───┘ └───┴───┴───┴───┘ ▼
 ◄───────────────────────────────────────────────────►
                    z: Num channels

Last note: for each sample we accumulate the results obtained for each filters.
However we still have a batch dimension at the end.

### Updating filters

Here again, for each sample for each filter, we perform a convolution between the actual image input
and the gradients. In the implementation I also use a specialized convolution function that takes a
3d matrix (sample i), a matrix (gradient for sample i for filter f) that produces an output of the
same shape as the filters (a 3d matrix).

So let's take the actual image input that were used as the example on our forward convolution:

 ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐ ┌───┬───┬───┬───┐
 │ 2 │ 2 │ 2 │ 2 │ │ 3 │ 0 │ 0 │ 3 │ │ 4 │ 0 │ 4 │ 4 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 2 │ 0 │ 0 │ 2 │ │ 0 │ 3 │ 3 │ 0 │ │ 0 │ 4 │ 0 │ 4 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 2 │ 0 │ 0 │ 2 │ │ 0 │ 3 │ 3 │ 0 │ │ 4 │ 0 │ 4 │ 0 │
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤
 │ 2 │ 2 │ 2 │ 2 │ │ 3 │ 0 │ 0 │ 3 │ │ 4 │ 4 │ 0 │ 4 │
 └───┴───┴───┴───┘ └───┴───┴───┴───┘ └───┴───┴───┴───┘

And the gradient for sample 0 for filter 0:

  This one ┌ ─ ┬ ─ ┐ ┌───┬───┐ ▲
  └───────►  5   0   │ 7 │ 0 │ │
           ├ ─ ┼ ─ ┤ ├───┼───┤ │
             0   1   │ 0 │ 0 │ │
           └ ─ ┴ ─ ┘ └───┴───┘ │ t: Batch size
           ┌───┬───┐ ┌───┬───┐ │
           │ 1 │ 0 │ │ 0 │ 1 │ │
           ├───┼───┤ ├───┼───┤ │
           │ 8 │ 0 │ │ 0 │ 6 │ │
           └───┴───┘ └───┴───┘ ▼
           ◄─────────────────►
             z: num filters

                         Input
 ┌ ─ ┬ ─ ┬───┬───┐ ┌ ─ ┬ ─ ┬───┬───┐ ┌ ─ ┬ ─ ┬───┬───┐                    dl_df
   2   2   2 │ 2 │   3   0   0 │ 3 │   4   0   4 │ 4 │  ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐
 ├ ─ ┼ ─ ┼───┼───┤ ├ ─ ┼ ─ ┼───┼───┤ ├ ─ ┼ ─ ┼───┼───┤  │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │
   2   0   0 │ 2 │   0   3   3 │ 0 │   0   4   0 │ 4 │  ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤
 ├ ─ ┼ ─ ┼───┼───┤ ├ ─ ┼ ─ ┼───┼───┤ ├ ─ ┼ ─ ┼───┼───┤  │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │
 │ 2 │ 0 │ 0 │ 2 │ │ 0 │ 3 │ 3 │ 0 │ │ 4 │ 0 │ 4 │ 0 │  ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤  │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │
 │ 2 │ 2 │ 2 │ 2 │ │ 3 │ 0 │ 0 │ 3 │ │ 4 │ 4 │ 0 │ 4 │  └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘
 └───┴───┴───┴───┘ └───┴───┴───┴───┘ └───┴───┴───┴───┘  

So here we compute dl_df(0,0, channel) like this:

                         Input
 ┌ ─ ┬ ─ ┬───┬───┐ ┌ ─ ┬ ─ ┬───┬───┐ ┌ ─ ┬ ─ ┬───┬───┐                    dl_df
  2x5 2x0  2 │ 2 │  3x5 0x0  0 │ 3 │  4x5 0x0  4 │ 4 │  ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐
 ├ ─ ┼ ─ ┼───┼───┤ ├ ─ ┼ ─ ┼───┼───┤ ├ ─ ┼ ─ ┼───┼───┤  │►10│ 0 │ 0 │ │►18│ 0 │ 0 │ │►24│ 0 │ 0 │
  2x0 0x1  0 │ 2 │  0x0 3x1  3 │ 0 │  0x0 4x1  0 │ 4 │  ├┼──┼───┼───┤ ├┼──┼───┼───┤ ├┼──┼───┼───┤
 ├ ─ ┼ ─ ┼───┼───┤ ├ ─ ┼ ─ ┼───┼───┤ ├ ─ ┼ ─ ┼───┼───┤  ││0 │ 0 │ 0 │ ││0 │ 0 │ 0 │ ││0 │ 0 │ 0 │
 │ 2 │ 0 │ 0 │ 2 │ │ 0 │ 3 │ 3 │ 0 │ │ 4 │ 0 │ 4 │ 0 │  ├┼──┼───┼───┤ ├┼──┼───┼───┤ ├┼──┼───┼───┤
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤  ││0 │ 0 │ 0 │ ││0 │ 0 │ 0 │ ││0 │ 0 │ 0 │
 │ 2 │ 2 │ 2 │ 2 │ │ 3 │ 0 │ 0 │ 3 │ │ 4 │ 4 │ 0 │ 4 │  └┼──┴───┴───┘ └┼──┴───┴───┘ └┼──┴───┴───┘
 └───┴───┴───┴───┘ └───┴───┴───┴───┘ └───┴───┴───┴───┘   │             │             │
  ─► 2x5 = 10       ─► 3x5+3x1 = 18   ─► 4x5+4x1 = 24    │             │             │
           │                     │                 └─────┼─────────────┼─────────────┘
           │                     └───────────────────────┼─────────────┘
           └─────────────────────────────────────────────┘

dl_df(1,0, channel):

 ┌───┬ ─ ┬ ─ ┬───┐ ┌───┬ ─ ┬ ─ ┬───┐ ┌───┬ ─ ┬ ─ ┬───┐                                            
 │ 2   2   2   2 │ │ 3   0   0   3 │ │ 4   0   4   4 │  ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐ 
 ├───┼ ─ ┼ ─ ┼───┤ ├───┼ ─ ┼ ─ ┼───┤ ├───┼ ─ ┼ ─ ┼───┤  │10 │10 │ 0 │ │18 │ 3 │ 0 │ │24 │24 │ 0 │ 
 │ 2   0   0   2 │ │ 0   3   3   0 │ │ 0   4   0   4 │  ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ 
 ├───┼ ─ ┼ ─ ┼───┤ ├───┼ ─ ┼ ─ ┼───┤ ├───┼ ─ ┼ ─ ┼───┤  │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 
 │ 2 │ 0 │ 0 │ 2 │ │ 0 │ 3 │ 3 │ 0 │ │ 4 │ 0 │ 4 │ 0 │  ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ 
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤  │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 
 │ 2 │ 2 │ 2 │ 2 │ │ 3 │ 0 │ 0 │ 3 │ │ 4 │ 4 │ 0 │ 4 │  └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ 
 └───┴───┴───┴───┘ └───┴───┴───┴───┘ └───┴───┴───┴───┘                                            
                                                                                                         
dl_df(2,0, channel):

 ┌───┬───┬ ─ ┬ ─ ┐ ┌───┬───┬ ─ ┬ ─ ┐ ┌───┬───┬ ─ ┬ ─ ┐                                                    
 │ 2 │ 2   2   2   │ 3 │ 0   0   3   │ 4 │ 0   4   4    ┌───┬───┬───┐ ┌───┬───┬───┐ ┌───┬───┬───┐        
 ├───┼───┼ ─ ┼ ─ ┤ ├───┼───┼ ─ ┼ ─ ┤ ├───┼───┼ ─ ┼ ─ ┤  │10 │10 │12 │ │18 │ 3 │ 0 │ │24 │24 │24 │         
 │ 2 │ 0   0   2   │ 0 │ 3   3   0   │ 0 │ 4   0   4    ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤        
 ├───┼───┼ ─ ┼ ─ ┤ ├───┼───┼ ─ ┼ ─ ┤ ├───┼───┼ ─ ┼ ─ ┤  │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 
 │ 2 │ 0 │ 0 │ 2 │ │ 0 │ 3 │ 3 │ 0 │ │ 4 │ 0 │ 4 │ 0 │  ├───┼───┼───┤ ├───┼───┼───┤ ├───┼───┼───┤ 
 ├───┼───┼───┼───┤ ├───┼───┼───┼───┤ ├───┼───┼───┼───┤  │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 
 │ 2 │ 2 │ 2 │ 2 │ │ 3 │ 0 │ 0 │ 3 │ │ 4 │ 4 │ 0 │ 4 │  └───┴───┴───┘ └───┴───┴───┘ └───┴───┴───┘ 
 └───┴───┴───┴───┘ └───┴───┴───┴───┘ └───┴───┴───┴───┘                                            
                                                                                                  
etc.

We repeat this step for each filter, accumulating the results in dl_df buffer.
Finally we scale dl_df by the learning rate and substract the result to the filters.

### Updating biases

As a reminder, biases are stored in a vector with size num_filters.

Here we simply have:

  biases
 ┌───┬───┐
 │-1 │ 1 │
 └───┴───┘

And to compute dl_db we have to sum our gradients over x, y, and t, which means that we sum
everything but the dimension z:

                     x: forward_output.x
                     ◄───────►
         ▲ ┌───┬───┐ ┌───┬───┐ ▲
         │ │ 5 │ 0 │ │ 7 │ 0 │ │
         │ ├───┼───┤ ├───┼───┤ │ y: forward_output.y
         │ │ 0 │ 1 │ │ 0 │ 0 │ │
t: Batch │ └───┴───┘ └───┴───┘ ▼
   size  │ ┌───┬───┐ ┌───┬───┐ 
         │ │ 1 │ 0 │ │ 0 │ 1 │ 
         │ ├───┼───┤ ├───┼───┤ 
         │ │ 8 │ 0 │ │ 0 │ 6 │ 
         ▼ └───┴───┘ └───┴───┘ 
           ◄─────────────────►                 
             z: num filters                    


         ▲ ┌───┐ ┌───┐ ▲
         │ │ 5 │ │ 7 │ │
         │ ├───┤ ├───┤ │ y: forward_output.y
         │ │ 1 │ │ 0 │ │
t: Batch │ └───┘ └───┘ ▼
   size  │ ┌───┐ ┌───┐ 
         │ │ 1 │ │ 1 │ 
         │ ├───┤ ├───┤ 
         │ │ 8 │ │ 6 │ 
         ▼ └───┘ └───┘ 
           ◄─────────►                 
           z: num filters                          

         ▲ ┌───┐ ┌───┐
         │ │ 6 │ │ 7 │  
t: Batch │ └───┘ └───┘  
   size  │ ┌───┐ ┌───┐
         │ │ 9 │ │ 7 │ 
         ▼ └───┘ └───┘ 
           ◄─────────►                 
           z: num filters                          
 
            ┌───┬───┐
      dl_db │15 │14 │
            └───┴───┘
            ◄───────►
            z: num filters


Then we scale dl_db by the learning rate, and substract it to biases.
```
